{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Szelestey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Szelestey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual description of the dataset and its characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a dataset (https://www.kaggle.com/datasets/lavagod/radiohead?select=radiohead.csv) containing the lyrics of the songs of the band Radiohead to analyze the semantic meaning of the songs from the perspective of data science. My key interest is to find correlations between the song lyrics and the song albums features.\n",
    "The dataset contains the following features for each song:\n",
    "- valence: The valence score of the song (from 0 to 1 how positive or negative the song is)\n",
    "- duration_ms: The duration of the song in mili-seconds\n",
    "- lyrics: The lyrics of the song\n",
    "- album_name: The name of the album the song belongs to\n",
    "- album_release_year: The year the album was released\n",
    "- album_img: The image url of the album cover\n",
    "- pct_sad: Gloom index of the song\n",
    "- word_count: The number of words in the lyrics\n",
    "- lyrical_density: Lyric Density is about how close together or far apart the words are over a given tempo. (? source: https://gypsy-folklores.tripod.com/id29.html)\n",
    "\n",
    "I want to find correlations between the lyrics words' frequencies, distributions and embeddings so I also need to preprocess the lyrics and create a new dataframe with the lyrics words' frequencies, distributions and embeddings. For that I will first tokenize the lyrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataset and creating a dataframe with the tokenized lyrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>valence</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>album_name</th>\n",
       "      <th>album_release_year</th>\n",
       "      <th>album_img</th>\n",
       "      <th>pct_sad</th>\n",
       "      <th>word_count</th>\n",
       "      <th>lyrical_density</th>\n",
       "      <th>gloom_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You</td>\n",
       "      <td>0.305</td>\n",
       "      <td>208667</td>\n",
       "      <td>you are the sun and moon and stars are you and...</td>\n",
       "      <td>Pablo Honey</td>\n",
       "      <td>1993</td>\n",
       "      <td>https://i.scdn.co/image/e17011b2aa33289dfa6c08...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>19</td>\n",
       "      <td>0.091054</td>\n",
       "      <td>50.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creep</td>\n",
       "      <td>0.096</td>\n",
       "      <td>238640</td>\n",
       "      <td>when you were here before couldn't look you in...</td>\n",
       "      <td>Pablo Honey</td>\n",
       "      <td>1993</td>\n",
       "      <td>https://i.scdn.co/image/e17011b2aa33289dfa6c08...</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>51</td>\n",
       "      <td>0.213711</td>\n",
       "      <td>22.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  track_name  valence  duration_ms  \\\n",
       "0        You    0.305       208667   \n",
       "1      Creep    0.096       238640   \n",
       "\n",
       "                                              lyrics   album_name  \\\n",
       "0  you are the sun and moon and stars are you and...  Pablo Honey   \n",
       "1  when you were here before couldn't look you in...  Pablo Honey   \n",
       "\n",
       "   album_release_year                                          album_img  \\\n",
       "0                1993  https://i.scdn.co/image/e17011b2aa33289dfa6c08...   \n",
       "1                1993  https://i.scdn.co/image/e17011b2aa33289dfa6c08...   \n",
       "\n",
       "   pct_sad  word_count  lyrical_density  gloom_index  \n",
       "0   0.0000          19         0.091054        50.39  \n",
       "1   0.0784          51         0.213711        22.60  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(Path('data/radiohead.csv'), encoding='latin1')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "track_name             object\n",
       "valence               float64\n",
       "duration_ms             int64\n",
       "lyrics                 object\n",
       "album_name             object\n",
       "album_release_year      int64\n",
       "album_img              object\n",
       "pct_sad               float64\n",
       "word_count              int64\n",
       "lyrical_density       float64\n",
       "gloom_index           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>valence</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>album_name</th>\n",
       "      <th>album_release_year</th>\n",
       "      <th>album_img</th>\n",
       "      <th>pct_sad</th>\n",
       "      <th>word_count</th>\n",
       "      <th>lyrical_density</th>\n",
       "      <th>gloom_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [track_name, valence, duration_ms, lyrics, album_name, album_release_year, album_img, pct_sad, word_count, lyrical_density, gloom_index]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "track_name            101\n",
       "valence               101\n",
       "duration_ms           101\n",
       "lyrics                 98\n",
       "album_name            101\n",
       "album_release_year    101\n",
       "album_img             101\n",
       "pct_sad               101\n",
       "word_count            101\n",
       "lyrical_density       101\n",
       "gloom_index           101\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have 3 words with no lyrics, this make them unusable for the analysis so I will drop them\n",
    "df_lyrics = df.dropna()\n",
    "df_lyrics.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the lyrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the lyrics and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['could',\n",
       " 'look',\n",
       " 'eye',\n",
       " 'like',\n",
       " 'angel',\n",
       " 'skin',\n",
       " 'makes',\n",
       " 'cry',\n",
       " 'float',\n",
       " 'like',\n",
       " 'feather',\n",
       " 'beautiful',\n",
       " 'world',\n",
       " 'chorus',\n",
       " 'wish',\n",
       " 'special',\n",
       " 'fuckin',\n",
       " 'special',\n",
       " 'creep',\n",
       " 'weirdo']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing whitespace\n",
    "ws_removed = [re.sub(r'\\s+', ' ', lyric) for lyric in list(df_lyrics['lyrics'])]\n",
    "\n",
    "# Tokenizing\n",
    "tokens = [word_tokenize(lyric) for lyric in ws_removed]\n",
    "\n",
    "# Stopword removal\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokens_no_sw = [[word for word in lyric if word not in stopwords] for lyric in tokens]\n",
    "\n",
    "# Filtering out words with special characters\n",
    "tokens_cleaned = [[word for word in lyric if re.match(r'^[a-zA-Z]+$', word)] for lyric in tokens_no_sw]\n",
    "\n",
    "tokens_cleaned[1][:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering Low-Information words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('judge', 1.309990708191592),\n",
       " ('everyone', 1.297273432703542),\n",
       " ('slow', 1.2226579943121525),\n",
       " ('ripcord', 1.1960784726966711),\n",
       " ('arms', 1.1146891167222073),\n",
       " ('haunt', 1.1115072675565023),\n",
       " ('prove', 1.10378846708736),\n",
       " ('case', 0.9841384661888941),\n",
       " ('bulletproof', 0.8674262797484867),\n",
       " ('uptight', 0.8490680516056615)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate TF-IDF scores\n",
    "# This helps identify words that are particularly significant to specific documents rather than just commonly occurring words in general.\n",
    "num_docs = len(tokens_cleaned)\n",
    "tfidf_scores = {}\n",
    "words = {word for doc in tokens_cleaned for word in doc}\n",
    "\n",
    "# Calculate document frequency (number of documents containing the word)\n",
    "doc_freq = {word: sum(1 for doc in tokens_cleaned if word in doc) for word in words}\n",
    "\n",
    "for doc in tokens_cleaned:\n",
    "    word_counts = Counter(doc)\n",
    "    # Calculate term frequency (number of times the word appears in the document)\n",
    "    tf = {word: word_counts[word] / len(doc) for word in word_counts}\n",
    "    # Calculate inverse document frequency (log of total docs / doc frequency)\n",
    "    idf = {word: math.log(num_docs / doc_freq[word]) for word in tf}\n",
    "    # TF-IDF = term frequency * inverse document frequency\n",
    "    tfidf = {word: tf[word] * idf[word] for word in tf}\n",
    "    tfidf_scores.update(tfidf)\n",
    "\n",
    "# Convert dictionary to list of tuples and sort\n",
    "sorted_tfidf = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 10 words with the highest TF-IDF scores\n",
    "sorted_tfidf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['could', 'like', 'like', 'chorus', 'hurts', 'around', 'chorus']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set TF-IDF threshold, I chose to filter out words with TF-IDF score lower than mean * 0.3\n",
    "tfidf_threshold = np.mean(list(tfidf_scores.values())) * 0.3\n",
    "\n",
    "# Filter out low information words based on TF-IDF scores\n",
    "tokens_relevant = [[word for word in doc if tfidf_scores[word] > tfidf_threshold] for doc in tokens_cleaned]\n",
    "tokens_irrelevant = [[word for word in doc if tfidf_scores[word] <= tfidf_threshold] for doc in tokens_cleaned]\n",
    "\n",
    "tokens_irrelevant[1][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>valence</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>album_name</th>\n",
       "      <th>album_release_year</th>\n",
       "      <th>album_img</th>\n",
       "      <th>pct_sad</th>\n",
       "      <th>word_count</th>\n",
       "      <th>lyrical_density</th>\n",
       "      <th>gloom_index</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You</td>\n",
       "      <td>0.305</td>\n",
       "      <td>208667</td>\n",
       "      <td>you are the sun and moon and stars are you and...</td>\n",
       "      <td>Pablo Honey</td>\n",
       "      <td>1993</td>\n",
       "      <td>https://i.scdn.co/image/e17011b2aa33289dfa6c08...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.091054</td>\n",
       "      <td>50.39</td>\n",
       "      <td>[you, are, the, sun, and, moon, and, stars, ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  track_name  valence  duration_ms  \\\n",
       "0        You    0.305       208667   \n",
       "\n",
       "                                              lyrics   album_name  \\\n",
       "0  you are the sun and moon and stars are you and...  Pablo Honey   \n",
       "\n",
       "   album_release_year                                          album_img  \\\n",
       "0                1993  https://i.scdn.co/image/e17011b2aa33289dfa6c08...   \n",
       "\n",
       "   pct_sad  word_count  lyrical_density  gloom_index  \\\n",
       "0      0.0          19         0.091054        50.39   \n",
       "\n",
       "                                              tokens  \n",
       "0  [you, are, the, sun, and, moon, and, stars, ar...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w_tokens = df_lyrics.copy()\n",
    "df_w_tokens['tokens'] = tokens\n",
    "df_w_tokens.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read embeddings for tokens then reduce dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to analise the word choices of the songs based on their semantic meaning and turn them into more interpretable data. For that I will turn the tokens into embeddings and reduce their dimensionality. These steps might not be the most rational and efficient way of classifying songs and are more of a experimantational approach.\n",
    "\n",
    "For the embeddings I will use the  model, which is not considered a state-of-the-art embedding model but is a light-weight and efficient one which is easy to use and does the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't have to execute this section I will include the calculated embeddings in a seperate file\n",
    "if pretrained_model_path is None:\n",
    "    pretrained_model_path = Path('data/GoogleNews-vectors-negative300.bin.gz') \n",
    "    word_vectors = KeyedVectors.load_word2vec_format(pretrained_model_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have vectors with dimensionality of 300\n",
    "word_vectors['angle'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = { word: word_vectors[word] for word in words if word in word_vectors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1782\n",
      "1755\n",
      "{'ooooohh', 'efil', 'skwerking', 'hooooooo', 'touchall', 'mmmhm', 'favours', 'lundy', 'liffey', 'godrich', 'gnihtyreve', 'belisha', 'grey', 'mephistopheles', 'backdrifting', 'headshrinkers', 'colours', 'favourite', 'fastnet', 'aeroplane', 'stepford', 'ohhhhhhhhhhh', 'backdrifters', 'aaaaaaaaah', 'ansaphone', 'mould', 'hammerheaded'}\n"
     ]
    }
   ],
   "source": [
    "# 300 words weren't present in the pretrained embedding set\n",
    "print(len(words))\n",
    "print(len(vector_store.keys()))\n",
    "print(words - vector_store.keys()) # ezekért tényleg nem kár, mondjuk a grey az felettébb érdekes, hogy nem szerepel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert vector_store dictionary to DataFrame\n",
    "embeddings_df = pd.DataFrame.from_dict(vector_store, orient='index')\n",
    "\n",
    "# Save to CSV file\n",
    "embeddings_df.to_csv(Path('data/embeddings.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radiohead-semantic-statistics--vhIL8dc-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
